{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8007a334-61ad-4c43-83fd-cd5461916789",
   "metadata": {},
   "source": [
    "<a href=\"https://atap.edu.au\"><img src=\"https://www.atap.edu.au/atap-logo.png\" width=\"125\" height=\"50\" align=\"right\"></a>\n",
    "# ATAP: TopSBM\n",
    "\n",
    "*The [Australian Text Analytics Platform (ATAP)](https://www.atap.edu.au) is an open source environment that provides researchers with tools and training for analysing, processing, and exploring text. ATAP: TopSBM is an effort to integrate an approach to topic modelling based on stochastic block models developed by Altmann and colleagues (for references and further details see: [https://topsbm.github.io](https://topsbm.github.io))*\n",
    "\n",
    "---\n",
    "\n",
    "**TopSBM** is a topic modelling algorithm. [Topic modelling](https://en.wikipedia.org/wiki/Topic_model) find *topics* within a collection of documents.\n",
    "\n",
    "Put simply, a *topic* in topic modelling typically refers to groups of co-occurring words in documents, which are then assigned a label which describes the group or ‘topic’. The step of assigning a label to describe the group (the ‘topic’) is not part of the topic modelling algorithm and requires additional research into the relevant ‘topics’ and what kind of information each may characterise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf35796-e58e-4e25-915e-050a475e5d3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <span>For any questions, feedback, and/or comments about the tool, please contact the Sydney Informatics Hub at <a href=\"mailto:sih.info@sydney.edu.au\">sih.info@sydney.edu.au</a>.</span>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <span style=\"font-weight: bold;\">Jupyter Notebook User Guide</span>\n",
    "    <br>\n",
    "    <span>\n",
    "        If you are new to Jupyter Notebook, feel free to take a quick look at <a href=\"https://github.com/Australian-Text-Analytics-Platform/semantic-tagger/blob/main/documents/jupyter-notebook-guide.pdf\">this user guide</a> for basic information on how to use a notebook.\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d074772-0fcf-4d7a-9614-cbdfa7797820",
   "metadata": {},
   "source": [
    "## 1. Upload your dataset using the Corpus Loader\n",
    "\n",
    "In the Corpus Loader below, select your dataset and build it as a Corpus.\n",
    "\n",
    "This is the first step in using the TopSBM notebook. Your Corpus should contain a collection of documents, so that *topics* may be inferred by running the TopSBM algorithm.\n",
    "\n",
    "#### Instructions on using the Corpus Loader\n",
    "1. Upload your corpus files using the file browser on the left - ensure the files are in the directory \"corpus_files\". You can simply **drag and drop** your files. Clicking on the folder icon will show you the file explorer pane. Wait until your corpus has uploaded before you return to the notebook.\n",
    "2. Executing the following code cell then makes available the ATAP Corpus Loader.\n",
    "3. Load your files by selecting the files in the selector window and clicking the 'Load as corpus' button. Then select the right datatype label for your file contents. For example, if your file consists of text, the datatype TEXT is appropriate and no changes are necessary. The Corpus Loader also automatically creates and includes filename and filepath as TEXT data.\n",
    "4. Give your corpus a name (optional) and click on the button “Build corpus”. Wait until you receive the message “Corpus … built successfully”. Review your corpus in the Corpus Overview or continue immediately to the next code cell in the notebook.\n",
    "\n",
    "For detailed instructions on how to use the Corpus Loader, including uploading your own datasets, please click <a href=\"Corpus Loader User Guide.pdf\" target=\"_blank\">here</a> to open the instructions PDF.\n",
    "\n",
    "\n",
    "#### Sample datasets\n",
    "+ There are 4 sample datasets: `corpus.csv`, `wiki.csv`, `constitutions.csv`, `arxiv.csv`.\n",
    "\n",
    "    \n",
    "| Dataset      | Description                      | # documents|\n",
    "|:--------------|----------------------------------|-----|\n",
    "| `corpus.csv`    | sourced from [wikipedia](https://wikipedia.org) | 63 |\n",
    "| `wiki.csv`    | sourced from [wikipedia](https://wikipedia.org)       | 120 |\n",
    "| `constitutions.csv`    |   constitutions of various countries       |189 |\n",
    "| `arxiv.csv`    |     sourced from [arxiv.org](https://arxiv.org)    | 2539|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a1902-3649-4b5d-82b9-5996a0302c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atap_corpus_loader import CorpusLoader\n",
    "\n",
    "loader = CorpusLoader('corpus_files')\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791719e-6b40-4b3b-9037-2fa062d1a4b7",
   "metadata": {},
   "source": [
    "Run the cell below to set the last corpus you've uploaded to the loader to be used for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3df49-bce0-40e9-8d8f-e0d053768954",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = loader.get_latest_corpus()\n",
    "f\"Your selected corpus: name={corpus.name} #documents={len(corpus)} metadata: {', '.join(corpus.metas)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da93c63-7123-47c4-bf0f-83666599e63e",
   "metadata": {},
   "source": [
    "## 2. Breaking up your documents into words using a whitespace tokeniser\n",
    "\n",
    "<br>\n",
    "\n",
    "> **In order for TopSBM to infer topics from your documents, you have to break up the *'words'* or *'tokens'* (as the technical term) in your documents.**\n",
    "\n",
    "<br/>\n",
    "The most common method of tokenisation is to use whitespace as the delimiter.<br><br>\n",
    "\n",
    "We will be using the default tokeniser from [spacy](https://spacy.io/usage/spacy-101#annotations-token) (a popular NLP python library) to break the documents up into words.\n",
    "It is a more intelligent whitespace tokeniser as it can handle nuances common in text data.\n",
    "<br><br>\n",
    "e.g. \"A fox jumped over a lazy fox and it didn't mind at all.\" will be broken up into \"A\", \"fox\", \"jumped\", \"over\", \"a\", \"lazy\", \"fox\", \"and\", \"it\", \"did\", \"n't\", \"mind\", \"at\", \"all\", \".\".\n",
    "<br><br>\n",
    "Notice that \"all.\" is broken up into \"all\" and \".\", and contractions like \"didn't\" is broken up into \"did\", \"n't\".\n",
    "<br><br>\n",
    "*Below are some more examples:*\n",
    "| sentence | tokens |\n",
    "|--------------|---------------------------------- |\n",
    "|Apple is looking at buying U.K. startup for \\$1 billion|\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \"for\", \"$\", \"1\", \"billion\"|\n",
    "|Autonomous cars shift insurance liability toward manufacturers|\"Autonomous\", \"cars\", \"shift\", \"insurance\", \"liability\", \"toward\", \"manufacturers\"|\n",
    "|San Francisco considers banning sidewalk delivery robots|\"San\", \"Francisco\", \"considers\", \"banning\", \"sidewalk\", \"delivery\", \"robots\"|\n",
    "|London is a big city in the United Kingdom.|\"London\", \"is\", \"a\", \"big\", \"city\", \"in\", \"the\", \"United\", \"Kingdom\", \".\"|\n",
    "|Where are you?|\"Where\", \"are\", \"you\", \"?\"|\n",
    "|Who is the president of France?|\"Who\", \"is\", \"the\", \"president\", \"of\", \"France\", \"?\"|\n",
    "|What is the capital of the United States?|\"What\", \"is\", \"the\", \"capital\", \"of\", \"the\", \"United\", \"States\", \"?\"|\n",
    "|When was Barack Obama born?|\"When\", \"was\", \"Barack\", \"Obama\", \"born\", \"?\"|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1684d9-83c6-4633-aea9-43d586e4ade0",
   "metadata": {},
   "source": [
    "In order to use spacy, we must first modify the documents in our Corpus to 'spacy' documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2effd-d97f-4ebf-93ad-b84de036cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank('en')\n",
    "nlp.max_length = 1_500_000  # increase to support long articles up to 1.5 million characters\n",
    "corpus.run_spacy(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c2b54-7375-4599-9181-5b0e6d9ca2ae",
   "metadata": {},
   "source": [
    "Tokenisers are defined here:\n",
    "1. `whitespace` - words are broken up by whitespace.\n",
    "2. `whitespace_lower_case_only` - words are broken up by whitespace and are lower cased.\n",
    "3. `lemmas` - words are broken up by whitespace and are lemmatised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b71c64-97a5-4a16-bbed-8a34d3667876",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenisers = {\n",
    "    \"whitespace\": lambda doc: [word.text for word in doc],\n",
    "    \"whitespace_lower_case_only\": lambda doc: [word.text.lower() for word in doc],\n",
    "    \"lemmas\": lambda doc: [word.lemma_ for word in doc],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d77d8-f07c-4709-aee7-cfa9992723ae",
   "metadata": {},
   "source": [
    "### [Recommended] Filter out special tokens\n",
    "\n",
    "You may have exported the text from a source that may contain IDs, phone numbers etc. These *special* textual elements can be detrimental to the model's output. \n",
    "\n",
    "If you believe these textual elements in your corpus can be used to identify the topics, you can remove this filter in a later cell below in section 3. TopSBM. You'll also find the instructions to remove the filter just before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ede05-195a-46ef-bdda-a18f65710e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_ALPHA\": True}]  # Match words only\n",
    "matcher.add(\"WORDS_ONLY\", [pattern])\n",
    "\n",
    "filters = dict()\n",
    "filters[\"no_special_tokens\"] = matcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01add3d-ecf8-4f57-890f-373720a9c566",
   "metadata": {},
   "source": [
    "### [Optional] Filtering out stop words\n",
    "\n",
    "Sometimes you might want to remove words from your documents that are highly frequent and are commonly excluded from topic modelling and other NLP applications.\n",
    "\n",
    "Such as, 'the', 'of', 'also', 'am', etc. These words are also known as 'stop words'.\n",
    "\n",
    "Filters are defined here:\n",
    "1. `no_stopwords` - stop words are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c1c7b-a591-4b1b-bbb5-58a17194c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"IS_STOP\": False}]  # Match tokens that are not stopwords\n",
    "matcher.add(\"NON_STOPWORDS\", [pattern])\n",
    "\n",
    "filters[\"no_stopwords\"] = matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e198-793b-4530-9d47-062f530ed18d",
   "metadata": {},
   "source": [
    "Run the cell below to show the list of stop words used by spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c4690-c117-4fcd-943b-579958f65b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "stop_words: str = ', '.join(sorted(set(nlp.Defaults.stop_words), reverse=False))\n",
    "pn.widgets.StaticText(name='SpaCy stop words: ', value=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae7bcf-2369-40bd-81aa-dd1ffca47c3d",
   "metadata": {},
   "source": [
    "## 3. TopSBM\n",
    "\n",
    "The following section runs the TopSBM algorithm on your Corpus.\n",
    "\n",
    "First, we will extract the list of words from your Corpus using the `tokenisers` and `filters` we defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1937a2a-9553-4e8d-8a3d-29e02d2643ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available tokenisers: {', '.join(tokenisers.keys())}\")\n",
    "print(f\"Available filters: {', '.join(filters.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25397f26-c6f6-44a1-bfff-d7b7e4fbb4b8",
   "metadata": {},
   "source": [
    "**To change to another available tokeniser**, edit the code cell below before you run it. <br>\n",
    "The following example will use the *lemmas* tokeniser.<br/>\n",
    "```python\n",
    "list_of_words =  atap.to_list_of_words(\n",
    "    corpus, \n",
    "    tokenisers['lemmas'],                            <-- see here\n",
    "    filters['no_special_tokens'],\n",
    ")\n",
    "```\n",
    "\n",
    "**To add a filter**, edit the code cell below before you run it.<br>\n",
    "The following example adds the *no_stopwords* filter.<br/>\n",
    "```python\n",
    "list_of_words =  atap.to_list_of_words(\n",
    "    corpus, \n",
    "    tokenisers['whitespace_lower_case_only'], \n",
    "    filters['no_special_tokens'],\n",
    "    filters['no_stopwords'],                          <-- see here\n",
    ")\n",
    "```\n",
    "\n",
    "**To remove a filter**, simply remove the line before you run it.<br/>\n",
    "```python\n",
    "list_of_words =  atap.to_list_of_words(\n",
    "    corpus, \n",
    "    tokenisers['whitespace_lower_case_only'], \n",
    "    filters['no_special_tokens'],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f111d07-4e0a-4516-8f77-e262bc6ebbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atap_wrapper as atap\n",
    "\n",
    "list_of_words =  atap.to_list_of_words(\n",
    "    corpus, \n",
    "    tokenisers['whitespace_lower_case_only'], \n",
    "    filters['no_special_tokens'],\n",
    ")\n",
    "titles = corpus['title'].tolist() if 'title' in corpus.metas else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3209ac-992f-4af7-a60c-53fd95ce04df",
   "metadata": {},
   "source": [
    "1. `atap.set_seed(42)` ensures reproducible results when the same seed is used again.\n",
    "2. `model.make_graph(...)` constructs the graph from your list of words. (optionally provide 'titles' of your documents)\n",
    "3. `model.fit()` will then run the TopSBM algorithm.\n",
    "\n",
    "Run the cell below to start fitting the model. If you have a large dataset (Corpus), this can take a while, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8924d-72ea-4362-8bc7-4a2e43304197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atap_wrapper as atap\n",
    "from topsbm.sbmtm import sbmtm\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "atap.set_seed(42)\n",
    "\n",
    "spinner = pn.indicators.LoadingSpinner(value=True, name='Fitting model...', color='success')\n",
    "display(spinner)\n",
    "\n",
    "model = sbmtm()\n",
    "model.make_graph(\n",
    "    list_of_words,\n",
    "    titles,\n",
    ")\n",
    "model.fit()\n",
    "\n",
    "spinner.value=False\n",
    "spinner.name=\"Fitting complete.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b9e75-9ed4-43f4-b62e-751a042a59e9",
   "metadata": {},
   "source": [
    "## 4. Visualise Outputs in a Radial Cluster\n",
    "\n",
    "Now that the TopSBM has been fitted onto your dataset, you can now visualise the outputs.\n",
    "\n",
    "There are currently 2 visualisations for the model. \n",
    "\n",
    "1. visualise the document groups belonging to the same topics.\n",
    "2. visualise the word groups (i.e. topics) that have been formed for the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599d55f-cf36-44e7-8ff5-e52ea72760dc",
   "metadata": {},
   "source": [
    "### 4a. Topics (groups of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19533d-bdeb-48d3-bbaf-8478bfdd6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_doc = atap.visualise(\n",
    "    model=model,   # our topsbm model\n",
    "    corpus=corpus, # our corpus \n",
    "    kind='documents', # visualise documents\n",
    "    width=1000,  # image width in pixels\n",
    "    height=1000, # image height in pixels\n",
    "    hierarchy='radial', # use radial cluster\n",
    "    categories=corpus['category'].tolist() if 'category' in corpus.metas else None,  # optional - category metadata used for the visualisation\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa47f50-87f4-48e6-99fa-ae59ca18651d",
   "metadata": {},
   "source": [
    "TopSBM is hierarchical, this means it infers topics, here as document groups, in increasing ***levels** of granularity.\n",
    "\n",
    "A document is assigned to a group at the most granular level, level 0. This document group is then assigned to a group at level 1. This process repeats until the maximum level as inferred by the model.\n",
    "\n",
    "The grouping in various levels is the same as having categories, sub-categories and sub-sub-categories.\n",
    "\n",
    "**You can change the `max_level` argument to merge document groups together based on their levels.**\n",
    "+ `max_level = 0` - show all levels.\n",
    "+ `max_level = 1` - show up to level 1 (0 being the most granular or lowest level)\n",
    "+ `max_level = 2` - show up to level 2 (0 being the most granular or lowest level)\n",
    "\n",
    "An error will be raised if `max_level` is larger than the maximum number of levels inferred by TopSBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4f790-01ea-44d1-aea4-0c765e8eecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_doc.display(max_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d55ca-df68-4358-9c73-00f217fb5dce",
   "metadata": {},
   "source": [
    "### 4b. Topics (groups of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a4e90-e76b-46dc-a290-0c33cd4fbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_words = atap.visualise(\n",
    "    model=model,  # our topsbm model\n",
    "    corpus=corpus, # our corpus\n",
    "    width=1000,  # image width in pixels\n",
    "    height=1000, # image height in pixels\n",
    "    kind='words',  # visualise words\n",
    "    hierarchy='radial', # use radial clluster\n",
    "    top_words_for_level=0, # only select the most probable words of this level\n",
    "    top_num_words=15, # select the top 'n' most probable words for level specified in top_words_for_level\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9eb857-6f33-438e-be67-acb4d5f4d56e",
   "metadata": {},
   "source": [
    "TopSBM is hierarchical, this means it infers topics, here as word groups, in increasing ***levels** of granularity.\n",
    "\n",
    "A word is assigned to a group at the most granular level, level 0. This word group is then assigned to a group at level 1. This process repeats until the maximum level as inferred by the model.\n",
    "\n",
    "The grouping in various levels is the same as having categories, sub-categories and sub-sub-categories.\n",
    "\n",
    "**You can change the `max_level` argument to merge word groups together based on their levels.**\n",
    "+ `max_level = 0` - show all levels.\n",
    "+ `max_level = 1` - show up to level 1 (0 being the outer most or lowest level)\n",
    "+ `max_level = 2` - show up to level 2 (0 being the outer most or lowest level)\n",
    "\n",
    "An error will be raised if `max_level` is larger than the maximum number of levels inferred by TopSBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307851a-a8fb-4d52-bb10-28bd596cfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_words.display(max_level=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "607676f9-4412-4e20-b30e-88729ed122cb",
   "metadata": {},
   "source": [
    "# 5. More technical: Group membership\n",
    "*The following sets of analyses are more technical explorations of the topics in your dataset, which provide additional insights.*\n",
    "\n",
    "<br>\n",
    "In the stochastic block model, word (-nodes) and document (-nodes) are clustered into different groups.\n",
    "\n",
    "The group membership can be represented by the conditional probability $P(\\text{group}\\, |\\, \\text{node})$. Since words and documents belong to different groups (the word-document network is bipartite) we can show separately:\n",
    "\n",
    "- $P(bd | d)$ - the probability of document $d$ to belong to document group $bd$\n",
    "- $P(bw | w)$ - the probability of word $w$ to belong to word group $bw$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549150d-1b00-4cd8-b5ec-06e6b225a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import pylab as plt\n",
    "\n",
    "p_td_d,p_tw_w = model.group_membership(l=1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(p_td_d,origin='lower',aspect='auto',interpolation='none')\n",
    "plt.title(r'Document group membership $P(bd | d)$')\n",
    "plt.xlabel('Document d (index)')\n",
    "plt.ylabel('Document group, bd')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(p_tw_w,origin='lower',aspect='auto',interpolation='none')\n",
    "plt.title(r'Word group membership $P(bw | w)$')\n",
    "plt.xlabel('Word w (index)')\n",
    "plt.ylabel('Word group, bw')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1f453-a385-4c6a-a468-be6fe783ef91",
   "metadata": {},
   "source": [
    "## Relative Topic Distribution\n",
    "Compare the frequency $f^i_d$ of words from topic $i$ in document $d$ with the expected value across all documents:\n",
    "\n",
    "$$ \\tau_d^i = (f^i_d -\\langle f^i \\rangle ) / \\langle f^i \\rangle $$\n",
    "\n",
    "as in Eq. (10) of [Hyland et al](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00288-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cf99e-3831-4e40-897a-4e27e2a0c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463c255-e360-460c-9ba5-8a90e5a36c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics (group of words)\n",
    "model.topics(l=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd28ef-f601-480b-b543-cfd196811bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative Document-Topic Contributions\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "\n",
    "tau_d=model.topicdist_relative(l=2)\n",
    "num_topics = len(tau_d[0])\n",
    "\n",
    "rows = [[model.documents[i]] + [tau_d[i][j] for j in range(num_topics)] for i in range(len(model.documents))]\n",
    "df = pd.DataFrame(rows, columns=[\"Document\"] + [f\"topic {i}\" for i in range(num_topics)])\n",
    "\n",
    "print(\"Document's relative contribution to each topic\")\n",
    "pn.widgets.DataFrame(df, show_index=False, autosize_mode='fit_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542ccce-d2fb-43bd-9fb6-a964e1092f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top contributing documents for each topic.\n",
    "\n",
    "top_docs=5\n",
    "model.docs_of_topic(l=2, n=top_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51196d-fbfc-4826-af54-14f637ec66b7",
   "metadata": {},
   "source": [
    "# 6. Export TopSBM's results for your Corpus\n",
    "\n",
    "> First, we'll add the TopSBM results from your analysis in new metadata columns at the document-level to your corpus.\n",
    "> These columns will contain the topics that each document belongs to for each level as inferred by TopSBM.\n",
    "\n",
    "\n",
    "> Then, we'll add some additional metadata your corpus at the corpus-level (we use the word 'attributes' to distinguish between corpus and document level metadata).<br>\n",
    "> The attributes describe the names of the new metadata columns added by your topsbm analysis, the git repository and the specific commit (i.e. a unique identifier of to a snapshot of the evolving repository) so that you can come back to the exact code you've used for your analysis.\n",
    "\n",
    "Finally, you can export the Corpus using Corpus Loader from before in the final code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566bf18-b433-4e2c-9758-77ab298f308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atap.add_results(model, corpus)\n",
    "print(\"Added new metadata columns and attributes to your corpus.\")\n",
    "\n",
    "print(f\"Your corpus now contains these metadata columns:\\n{', '.join(corpus.metas)}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "Your corpus now contains these attributes:\n",
    "\"\"\".strip())\n",
    "pn.pane.JSON(corpus.attributes, hover_preview=True, depth=-1, theme='light')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c3905-fff5-4062-b21c-bde8a614f4b6",
   "metadata": {},
   "source": [
    "**Export** the corpus using our corpus loader from before. You should be at the \"Corpus Overview\" tab where you can select the file type to export and the Export button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050d8b9-ca9f-43a1-8d17-83a4fdf30344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Export corpus you fitted the model on: i.e. name = '{corpus.name}'\")\n",
    "loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0457f862-82c0-412f-b459-85f9bcde2b5d",
   "metadata": {},
   "source": [
    "<a href=\"https://atap.edu.au\"><img src=\"https://www.atap.edu.au/atap-logo.png\" width=\"125\" height=\"50\" align=\"right\"></a>\n",
    "## Bring your Corpus with TopSBM results to other ATAP Tools!\n",
    "\n",
    "If you want to continue your analysis, you can take your corpus together with the associated TopSBM results (as metadata) and run further analyses with other ATAP tools.\n",
    "\n",
    "These tools will use the same ATAP Corpus Loader interface. You can find a collection of ATAP tools at https://www.atap.edu.au/tools/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
